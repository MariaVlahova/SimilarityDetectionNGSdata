{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #merge data\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "# path = 'Tuber_HIV_data/'\n",
    "# os.chdir(path)\n",
    "\n",
    "# all_filenames = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "\n",
    "# combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "# combined_csv.to_csv( \"tuberculasis_HIV.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv (r'C:\\workspace\\DataForRegressor.csv', index = False, header=True)\n",
    "import pandas as pd \n",
    "data1= pd.read_csv(\"data_Tuberculosis_HIV1.csv\")\n",
    "data1['comparison']='tuberculosis_HIV'\n",
    "data2= pd.read_csv(\"data_Tuberculosis_Covid19.csv\")\n",
    "data2['comparison']='tuberculosis_Covid19'\n",
    "data3= pd.read_csv(\"dat_HIV1_COVID19.csv\")\n",
    "data3['comparison']='HIV_COVID19'\n",
    "df = pd.concat([data1,data2])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=df.drop(['mean','STD','signalToNoise','comparison'], axis=1)\n",
    "data=df_data.dropna()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "def getRegressor(df,estimators,maxDepth,randomState):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    scaler = StandardScaler()  \n",
    "    X=df.copy()\n",
    "    y=df['global_score']\n",
    "    X_trainBase, X_testBase, y_train, y_test = train_test_split(X, y)\n",
    "    X_train=X_trainBase.drop(['chunk_or1','chunk_or2','global_score'], axis=1)\n",
    "    X_train=X_train.astype('int32')\n",
    "    X_train =scaler.fit_transform(X_train)\n",
    "    #X_train = scaler.transform(X_train)\n",
    "    X_test=X_testBase.drop(['chunk_or1','chunk_or2','global_score'], axis=1)\n",
    "    X_test=X_test.astype('int32')\n",
    "    X_test = scaler.transform(X_test)\n",
    "    model= XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.05, max_delta_step=0, max_depth=5,\n",
    "             min_child_weight=4, monotone_constraints='()',\n",
    "             n_estimators=600, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
    "             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n",
    "             scale_pos_weight=1, silent=1, subsample=0.7, tree_method='exact',\n",
    "             validate_parameters=1, verbosity=None).fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "    result = model.score(X_test,y_test)\n",
    "    end = time.time()\n",
    "    print(\"Score:\"+ str(result))\n",
    "    print(\"Time for executing model\")\n",
    "    print(end - start)\n",
    "    print(\" in seconds\")\n",
    "    return model,X_test,X_testBase,X_trainBase,X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07, 0.1], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7,8],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [200,300,500,600,700]}\n",
    "\n",
    "grid = GridSearchCV(model,parameters, cv=2,n_jobs = 5,\n",
    "                        verbose=True)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.05, max_delta_step=0, max_depth=5,\n",
    "             min_child_weight=4, monotone_constraints='()',\n",
    "             n_estimators=600, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
    "             objective='reg:linear', random_state=0, reg_alpha=0, reg_lambda=1,\n",
    "             scale_pos_weight=1, silent=1, subsample=0.7, tree_method='exact',\n",
    "             validate_parameters=1, verbosity=None)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#plt.rcParams['agg.path.chunksize'] = 10000\n",
    "test=pd.DataFrame(y_test)\n",
    "test['pred']=y_pred\n",
    "test['rank_distance']=X_testBase['rank_distance']\n",
    "dt=test[0:100]\n",
    "#dt.head()\n",
    "dt.plot(x='rank_distance', y=['global_score','pred'], figsize=(10,5), grid=True)\n",
    "# plt.scatter(dt['index'], y_test, color = \"red\")\n",
    "# plt.plot(dt['index'], y_pred, color = \"green\")\n",
    "# plt.title(\"Test\")\n",
    "# plt.xlabel(\"index\")\n",
    "# plt.ylabel(\"value\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression({'copy_X': True, 'fit_intercept': False, 'normalize': True}).fit(X_train, y_train)\n",
    "y_pred=reg.predict(X_test)\n",
    "regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor()\n",
    "param_grid = {'hidden_layer_sizes': [i for i in range(1,15)],\n",
    "              'activation': ['relu'],\n",
    "              'solver': ['adam'],\n",
    "              'learning_rate': ['constant'],\n",
    "              'learning_rate_init': [0.001],\n",
    "              'power_t': [0.5],\n",
    "              'alpha': [0.0001],\n",
    "              'max_iter': [1000],\n",
    "              'early_stopping': [False],\n",
    "              'warm_start': [False]}\n",
    "grid = GridSearchCV(mlp, param_grid=param_grid, verbose=True)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regr = MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "             hidden_layer_sizes=11, learning_rate='constant',\n",
    "             learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
    "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "             validation_fraction=0.1, verbose=False, warm_start=False).fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#plt.rcParams['agg.path.chunksize'] = 10000\n",
    "test=pd.DataFrame(y_test)\n",
    "test['pred']=y_pred\n",
    "test['rank_distance']=X_testBase['rank_distance']\n",
    "dt=test[0:100]\n",
    "#dt.head()\n",
    "dt.plot(x='rank_distance', y=['global_score','pred'], figsize=(10,5), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ml_metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from ml_metrics import rmse\n",
    "import numpy as np\n",
    "\n",
    "def regression_results(y_true, y_pred):# Regression metrics\n",
    "    explained_variance=explained_variance_score(y_true, y_pred)\n",
    "    mae=mean_absolute_error(y_true, y_pred) \n",
    "    mse=mean_squared_error(y_true, y_pred) \n",
    "    r2=r2_score(y_true, y_pred)\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mae,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,X_test,X_testBase,X_trainBase,X_train=getRegressor(data,200,20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificate result of train and test\n",
    "X_testBase['globalAlign_pred']=model.predict(X_test)\n",
    "X_trainBase['globalAlign_pred']=model.predict(X_train)\n",
    "prediction_test = pd.DataFrame(X_testBase, columns=['globalAlign_pred'])\n",
    "prediction_train = pd.DataFrame(X_trainBase, columns=['globalAlign_pred'])\n",
    "baseline_test = pd.DataFrame(X_testBase, columns=['global_score'])\n",
    "baseline_train = pd.DataFrame(X_trainBase, columns=['global_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calcMeanStdAndSignalToNoise(prediction,column):\n",
    "#     prediction['mean'] = prediction[column].rolling(window=3).mean()\n",
    "#     prediction['STD'] = prediction[column].rolling(window=3).std()\n",
    "#     prediction['signalToNoise']=prediction['mean']/prediction['STD']\n",
    "#     prediction=prediction.replace([np.inf, -np.inf], np.nan)\n",
    "#     prediction=prediction.fillna(0)\n",
    "#     return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_train=calcMeanStdAndSignalToNoise(prediction_train)\n",
    "# pred_test=calcMeanStdAndSignalToNoise(prediction_test)\n",
    "# base_train=calcMeanStdAndSignalToNoise(baseline_train)\n",
    "# base_test=calcMeanStdAndSignalToNoise(baseline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction['mean'] = data['globalAlign_pred'].rolling(window=3).mean()\n",
    "# prediction['STD'] = data['globalAlign_pred'].rolling(window=3).std()\n",
    "# prediction['signalToNoise']=data['mean']/data['STD']\n",
    "# prediction=prediction.replace([np.inf, -np.inf], np.nan)\n",
    "# prediction=prediction.fillna(0)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# prediction=scaler.fit_transform(prediction)\n",
    "# baseline['mean'] =baseline['global_score'].rolling(window=3).mean()\n",
    "# baseline['STD'] = baseline['global_score'].rolling(window=3).std()\n",
    "# baseline['signalToNoise']=baseline['mean']/baseline['STD']\n",
    "# baseline=baseline.replace([np.inf, -np.inf], np.nan)\n",
    "# baseline=baseline.fillna(0)\n",
    "# baseline=scaler.fit_transform(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_to_clusters(X_train,X_test):\n",
    "    from sklearn.cluster import KMeans\n",
    "    cluster = KMeans(n_clusters=5, init='k-means++', random_state=0)\n",
    "    cluster.fit_predict(X_train)\n",
    "    return cluster.predict(X_test),cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_cluster(labels_true, labels_pred,X_test):\n",
    "    import sklearn\n",
    "    ami_score=sklearn.metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "    compl_score=sklearn.metrics.completeness_score(labels_true, labels_pred)\n",
    "    adj_rand_score=sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "    calinski_harabasz_score=sklearn.metrics.calinski_harabasz_score(X_test, labels_pred)\n",
    "    davies_bouldin_score=sklearn.metrics.davies_bouldin_score(X_test, labels_pred)\n",
    "    fowlkes_mallows_score=sklearn.metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "    homogenity_score=sklearn.metrics.homogeneity_score(labels_true, labels_pred)\n",
    "    mutual_info_score=sklearn.metrics.mutual_info_score(labels_true, labels_pred)\n",
    "    silhouette_score=sklearn.metrics.silhouette_score(X_test, labels_pred)\n",
    "    print(\"**************************\")\n",
    "    print(\"ami_score: {}\".format(ami_score))\n",
    "    print(\"compl_score: {}\".format(compl_score))\n",
    "    print(\"adj_rand_score: {}\".format(adj_rand_score))\n",
    "    print(\"calinski_harabasz_score: {}\".format(calinski_harabasz_score))\n",
    "    print(\"davies_bouldin_score: {}\".format(davies_bouldin_score))\n",
    "    print(\"fowlkes_mallows_score: {}\".format(fowlkes_mallows_score))\n",
    "    print(\"homogenity_score: {}\".format(homogenity_score))\n",
    "    print(\"mutual_info_score: {}\".format(mutual_info_score))\n",
    "    print(\"silhouette_score: {}\".format(silhouette_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels,cluster_pred=map_data_to_clusters(prediction_train,prediction_test)\n",
    "baseline_labels,cluster_base=map_data_to_clusters(baseline_train,baseline_test)\n",
    "prediction_test = pd.DataFrame(X_testBase, columns=['globalAlign_pred'])\n",
    "prediction_train = pd.DataFrame(X_trainBase, columns=['globalAlign_pred'])\n",
    "baseline_test = pd.DataFrame(X_testBase, columns=['global_score'])\n",
    "baseline_train = pd.DataFrame(X_trainBase, columns=['global_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result analysis-test with hiv/covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df=data3.copy()\n",
    "scaler = StandardScaler()  \n",
    "X_hc=data3.copy()\n",
    "y_hc=df['global_score']\n",
    "X_test_hc=data3.drop(['chunk_or1','chunk_or2','global_score','mean','STD','signalToNoise','comparison'], axis=1)\n",
    "X_test_hc=X_test_hc.astype('int32')\n",
    "X_test_hc_scaled =scaler.fit_transform(X_test_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_hc['globalAlign_pred']\n",
    "cluster_data = pd.DataFrame(model.predict(X_test_hc_scaled), columns=['globalAlign_pred'], index=X_test_hc.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_hc['globalAlign_pred']=model.predict(X_test_hc_scaled)\n",
    "X_test_hc['clusters']=cluster_pred.predict(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_hc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_hc['chunk_or1']=data3['chunk_or1']\n",
    "X_test_hc['chunk_or2']=data3['chunk_or2']\n",
    "X_test_hc['comparison']=data3['comparison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gfg_csv_data = df.to_csv('result_HIV_COVID.csv', index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testM=X_test_hc[X_test_hc['chunk_or2'].str.contains('TGTTAAAGCCACTTGCGAATTTTGTGGCACTGAGAATTTGACTAAAGAAGGTGCCACTACTTGTGGTTACTTACCCCAAAATGCTGTTGTTAAAATTT', regex=False)]\n",
    "#testM[(testM.clusters==1)][['clusters','chunk_or1','chunk_or2','globalAlign_pred']]\n",
    "print(X_test_hc['chunk_or1'][166])\n",
    "print(X_test_hc['chunk_or2'][166])\n",
    "# testA=testM[testM['chunk_or1'].str.contains('', regex=False)]\n",
    "# testA[['clusters','chunk_or1','chunk_or2','globalAlign_pred']]\n",
    "# testA[(testA.clusters==1)][['clusters','chunk_or1','chunk_or2','globalAlign_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testM=X_test_hc[X_test_hc['chunk_or1'].str.contains('CATCCGGAGTACTTCAAGAAC', regex=False)]\n",
    "testM=testM[(testM.clusters==1)][['clusters','chunk_or1','chunk_or2','globalAlign_pred']]\n",
    "testA=testM[testM['chunk_or2'].str.contains('CTAAAGCTATTAA', regex=False)]\n",
    "#testA\n",
    "testA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_hc[(X_test_hc.clusters==1)][['chunk_or1','chunk_or2','globalAlign_pred']][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_hc['chunk_or2'][1].find('TCTCTCTGGTTAGACCAGATCT')  \n",
    "# start = 100 \n",
    "# end = 1000\n",
    "# any_string.index('substring', start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_hc[(X_test_hc.clusters==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['chunk_or2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'GATGGTGGCAGTTTGTATGTAAATAAACATGCATTCCACACACCAGCTTTTGATAAAAGT'in d['chunk_or2'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=X_test_hc[(X_test_hc.clusters==1)]\n",
    "d[['globalAlign_pred']].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cluster(baseline_labels, pred_labels,prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "labelsAB=map_data_to_clusters(baseline)\n",
    "labelsAF=map_data_to_clusters(data)\n",
    "#print(cluster_ab_1['data_index'])\n",
    "'''here manually decid ewhich cluster is with P example and which is with negative examples'''\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "score=adjusted_rand_score(labelsAB,labelsAF)\n",
    "print(score)\n",
    "NBClust(labelsAB,labelsAF,X_test)\n",
    "NBClust(labels_true, labels_pred,X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyeasyga\n",
    "import time\n",
    "from pyeasyga import pyeasyga\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual, data):\n",
    "    temp=0;\n",
    "    cNum=1\n",
    "    for x in data:\n",
    "        if temp < x['score']:\n",
    "            temp=x['score']\n",
    "            cNum=x['clusterNum']\n",
    "    return cNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data\n",
    "#print(X.head(5))\n",
    "start = time.time()\n",
    "#KMeans++\n",
    "arr=[];\n",
    "for i in range(2,20):\n",
    "    cluster_labels = KMeans(n_clusters=i, init='k-means++').fit_predict(X)\n",
    "    arr.append({'clusterNum': i , 'score': silhouette_score(X, cluster_labels)})\n",
    "    print({'clusterNum': i , 'score': silhouette_score(X, cluster_labels)})\n",
    "#initialise the GA with data\n",
    "ga = pyeasyga.GeneticAlgorithm(arr)\n",
    "ga.fitness_function = fitness               # set the GA's fitness function\n",
    "ga.run()                                    # run the GA\n",
    "print(ga.best_individual())\n",
    "end = time.time()\n",
    "print(\"Time for executing Kmeans++ with GA\")\n",
    "print(end - start)\n",
    "print(\" in seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBClust(labels_true, labels_pred,X_test):\n",
    "    import sklearn\n",
    "    ami_score=sklearn.metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n",
    "    compl_score=sklearn.metrics.completeness_score(labels_true, labels_pred)\n",
    "    adj_rand_score=sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "    calinski_harabasz_score=sklearn.metrics.calinski_harabasz_score(X_test, labels_pred)\n",
    "    davies_bouldin_score=sklearn.metrics.davies_bouldin_score(X_test, labels_pred)\n",
    "    fowlkes_mallows_score=sklearn.metrics.fowlkes_mallows_score(labels_true, labels_pred)\n",
    "    homogenity_score=sklearn.metrics.homogeneity_score(labels_true, labels_pred)\n",
    "    mutual_info_score=sklearn.metrics.mutual_info_score(labels_true, labels_pred)\n",
    "    silhouette_score=sklearn.metrics.silhouette_score(X_test, labels_pred)\n",
    "    print(\"**************************\")\n",
    "    print(\"ami_score: {}\".format(ami_score))\n",
    "    print(\"compl_score: {}\".format(compl_score))\n",
    "    print(\"adj_rand_score: {}\".format(adj_rand_score))\n",
    "    print(\"calinski_harabasz_score: {}\".format(calinski_harabasz_score))\n",
    "    print(\"davies_bouldin_score: {}\".format(davies_bouldin_score))\n",
    "    print(\"fowlkes_mallows_score: {}\".format(fowlkes_mallows_score))\n",
    "    print(\"homogenity_score: {}\".format(homogenity_score))\n",
    "    print(\"mutual_info_score: {}\".format(mutual_info_score))\n",
    "    print(\"Ð¼: {}\".format(silhouette_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
