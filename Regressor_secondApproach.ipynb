{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"********************Methods*********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LevenshteinDistance(s1, s2):\n",
    "    \"\"\"Computes the Levenshtein distance between two arrays (strings too).\n",
    "    Such distance is the minimum number of operations needed to transform one array into\n",
    "    the other, where an operation is an insertion, deletion, or substitution of a single\n",
    "    item (like a char). This implementation (Wagner-Fischer algorithm with just 2 lines)\n",
    "    uses O(min(|s1|, |s2|)) space.\n",
    "    \"\"\"\n",
    "    # This function is designed for Psyco\n",
    "    if s1 == s2: return 0 # this is fast in Python\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    r1 = list(range(len(s2) + 1))\n",
    "    r2 = [0] * len(r1)\n",
    "    i = 0\n",
    "    for c1 in s1:\n",
    "        r2[0] = i + 1\n",
    "        j = 0\n",
    "        for c2 in s2:\n",
    "            if c1 == c2:\n",
    "                r2[j+1] = r1[j]\n",
    "            else:\n",
    "                a1 = r2[j]\n",
    "                a2 = r1[j]\n",
    "                a3 = r1[j+1]\n",
    "                if a1 > a2:\n",
    "                    if a2 > a3:\n",
    "                        r2[j+1] = 1 + a3\n",
    "                    else:\n",
    "                        r2[j+1] = 1 + a2\n",
    "                else:\n",
    "                    if a1 > a3:\n",
    "                        r2[j+1] = 1 + a3\n",
    "                    else:\n",
    "                        r2[j+1] = 1 + a1\n",
    "            j += 1\n",
    "        aux = r1; r1 = r2; r2 = aux\n",
    "        i += 1\n",
    "    return r1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''if both seq are the same the score will be 0'''\n",
    "def rankDistance(seq1,seq2):\n",
    "    score=0\n",
    "    seq1_arr=list(seq1)\n",
    "    seq2_arr=list(seq2)\n",
    "\n",
    "    for i in range(len(seq1_arr)):\n",
    "        index=seq2_arr.index(seq1_arr[i]) if seq1_arr[i] in seq2_arr else 0\n",
    "        score=score+abs(i-index)\n",
    "        seq1_arr[i]='*'\n",
    "        seq2_arr[index]='*'\n",
    "    if(seq1_arr==seq2_arr):\n",
    "        for i in range(len(seq1_arr)):\n",
    "            j= i if seq1_arr[i]!='*' else 0\n",
    "            score=score+j\n",
    "            k= -i if seq2_arr[i]!='*' else 0\n",
    "            score=score+j\n",
    "    else:\n",
    "        for i in range(len(seq1_arr)):\n",
    "            j= i if seq1_arr[i]!='*' else 0\n",
    "            score=score+j\n",
    "        for i in range(len(seq2_arr)):\n",
    "            j= i if seq2_arr[i]!='*' else 0\n",
    "            score=score+j\n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#globalAlighnment\n",
    "'''\n",
    "for the calculation is used bio python\n",
    "gap_open_penalty and gap_extend_penalty neeed to have negative value\n",
    "'''\n",
    "def global_alighn_score(seq1,seq2,gap_open_penalty,gap_extend_penalty):\n",
    "    from Bio.SubsMat import MatrixInfo as matlist\n",
    "    from Bio import pairwise2\n",
    "    matrix = matlist.blosum62\n",
    "    a = pairwise2.align.globalds(seq1,seq2,matrix,gap_open_penalty,gap_extend_penalty)\n",
    "    (s1,s2,score,start,end) = a[0]\n",
    "    return score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitutionCost(a, b):\n",
    "    import numpy\n",
    "    import pandas\n",
    "    \"\"\"Return the cost of substituting character a with character b\"\"\"\n",
    "    return 0 if a == b else 3\n",
    "\n",
    "def calculateCostMatrix(a, b, gapCost=2):\n",
    "    import numpy\n",
    "    import pandas\n",
    "    \"\"\"For two sequences a, b, calculate their edit distance matrix\"\"\"\n",
    "    #Initialize n+1 x m+1 matrix. x axis <--> a, y axis <--> b\n",
    "    xdim = len(b) + 1\n",
    "    ydim = len(a) + 1\n",
    "    matrix = numpy.zeros((xdim, ydim))\n",
    "    #Initialize first row and first col with n*gap cost\n",
    "    matrix[0,:] = numpy.arange(0, gapCost*ydim, step=gapCost)\n",
    "    matrix[:,0] = numpy.arange(0, gapCost*xdim, step=gapCost)\n",
    "    #Fill array\n",
    "    for y in range(1, ydim):\n",
    "        for x in range(1, xdim):\n",
    "            substCost = matrix[x-1, y-1] + substitutionCost(b[x-1], a[y-1])\n",
    "            matrix[x,y] = min(substCost, matrix[x-1, y] + gapCost, matrix[x, y-1] + gapCost)\n",
    "    return matrix\n",
    "\n",
    "def centralStarDistance(a, b, gapCost=2):\n",
    "    import numpy\n",
    "    import pandas\n",
    "    \"\"\"Get the edit distance of two sequences a and b\"\"\"\n",
    "    return calculateCostMatrix(a, b, gapCost)[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''naighbourhood count'''\n",
    "def word_neighborhood_count_score(seq1, seq2):\n",
    "    score = 0\n",
    "    W=[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]\n",
    "    # for each base in the match\n",
    "    for i in range(0,len(seq1)):\n",
    "        counter=0\n",
    "        start=i\n",
    "        end=i+9\n",
    "        seq1_len=len(seq1)-1\n",
    "        seq2_len=len(seq2)-1\n",
    "        while start < end:\n",
    "            if(start >= seq1_len or start >= seq2_len):\n",
    "                score = score - W[counter]\n",
    "            elif(seq1[i] == seq2[i+counter]):\n",
    "                score = score + W[counter] \n",
    "                break\n",
    "            else:\n",
    "                score = score - W[counter]\n",
    "            counter = counter + 1\n",
    "            start=start+1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation(N,ratioComp): \n",
    "    import math\n",
    "    import time\n",
    "    from functools import reduce\n",
    "    mean=ratioComp.similarity.sum()/N\n",
    "    sum1=reduce(lambda x: (x -mean) / 2, ratioComp.similarity)\n",
    "    stand_dev=math.sqrt(pow(sum1, 2)/N)\n",
    "    signalToNoise=mean/stand_dev\n",
    "    for x in ratioComp:\n",
    "        x.mean=mean\n",
    "        x.standart_dev=standart_dev\n",
    "        x.signalToNoise=signalToNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*****************************DATA STRUCTURES**********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetInfo:\n",
    "    def __init__(self,first_chunk_value,\n",
    "                 second_chunk_value,word_count_score,rank_distance,levenshtein_distance,central_star_score,global_alighn_score):\n",
    "        self.first_chunk_value = first_chunk_value\n",
    "        self.second_chunk_value = second_chunk_value\n",
    "        self.word_count_score = word_count_score\n",
    "        self.rank_distance = rank_distance\n",
    "        self.levenshtein_distance = levenshtein_distance\n",
    "        self.central_star_score=central_star_score\n",
    "        self.global_alighn_score = global_alighn_score\n",
    "        self.mean = None \n",
    "        self.standart_dev = None\n",
    "        self.signalToNoise = None\n",
    "    def as_dict(self):\n",
    "        return {'chunk_or1': self.first_chunk_value,'chunk_or2': self.second_chunk_value,\n",
    "                'word_count': self.word_count_score,'rank_distance': self.rank_distance,\n",
    "                'levenshtein_distance':self.levenshtein_distance,'central_star_score':self.central_star_score,\n",
    "                'global_score': self.global_alighn_score,'mean': self.mean,\n",
    "                'STD':self.standart_dev,'signalToNoise':self.signalToNoise}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"******************************Load Data***************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "from Bio import SeqIO\n",
    "sequence_virus1 = \"\"\n",
    "for seq_record in SeqIO.parse(\"HIV.fasta\", \"fasta\"):\n",
    "    sequence_virus1=sequence_virus1+seq_record.seq\n",
    "sequence_virus2= \"\"\n",
    "for seq_record in SeqIO.parse(\"coronavirus.fasta\", \"fasta\"):\n",
    "    print(seq_record.seq)\n",
    "    sequence_virus2=sequence_virus2+seq_record.seq\n",
    "sequence_virus3= \"\"\n",
    "for seq_record in SeqIO.parse(\"tuberculosis.fasta\", \"fasta\"):\n",
    "    sequence_virus3=sequence_virus3+seq_record.seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=200\n",
    "window=40\n",
    "chunks_seq_virus1=[str(sequence_virus1[i:i+k]) for i in range(0, len(sequence_virus1), k)]\n",
    "chunks_seq_virus2=[str(sequence_virus2[i:i+k]) for i in range(0, len(sequence_virus2), k)]\n",
    "chunks_seq_virus3=[str(sequence_virus3[i:i+k]) for i in range(0, len(sequence_virus3), k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*******************************Preprocessing*******************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_preprocessing(seq1,seq2,gap_open_penalty,gap_extend_penalty,result):\n",
    "    global_score = global_alighn_score(seq1,seq2,gap_open_penalty,gap_extend_penalty)\n",
    "    word_count=word_neighborhood_count_score(seq1, seq2)\n",
    "    levenshtein_distance=LevenshteinDistance(seq1, seq2)\n",
    "    rankDistance=rank_distance(seq1, seq2)\n",
    "    central_star_score=centralStarDistance(seq1, seq2)\n",
    "    info =  DataSetInfo(seq1,seq2,word_count,rankDistance,levenshtein_distance,central_star_score,global_score)\n",
    "    result.put(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, queue\n",
    "  \n",
    "def paraller_preprocessing(chunks_seq_virus1,chunks_seq_virus2,gap_open_penalty,gap_extend_penalty):\n",
    "    q = queue.Queue()\n",
    "    threads = []\n",
    "    for x in range(int(len(chunks_seq_virus1))):\n",
    "        for y in range(int(len(chunks_seq_virus2))):\n",
    "            seq1 = chunks_seq_virus1[x]\n",
    "            seq2 = chunks_seq_virus2[y]\n",
    "            t=threading.Thread(target=chunk_preprocessing, args=(seq1,seq2,gap_open_penalty,gap_extend_penalty,q))\n",
    "            threads.append(t)\n",
    "            t.start()\n",
    "    for x in threads:\n",
    "        x.join()\n",
    "    result=list(q.queue)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install progressbar\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "from time import sleep\n",
    "bar = progressbar.ProgressBar(maxval=1000, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "l=int(len(chunks_seq_virus3)/1000)\n",
    "\n",
    "bar.start()\n",
    "for i in range(0, 501):\n",
    "    bar.update(i+1)\n",
    "    result=paraller_preprocessing(chunks_seq_virus1,chunks_seq_virus3[i*l:(i+1)*l],-10,-1)\n",
    "    dataFrame = pd.DataFrame([x.as_dict() for x in result])\n",
    "    filename=\"FullData_\"+str(i)+\".csv\"\n",
    "    dataFrame.to_csv(filename,index = False, header=True)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv (r'C:\\workspace\\DataForRegressor.csv', index = False, header=True)\n",
    "import pandas as pd \n",
    "baseline_data = pd.read_csv(\"C:\\workspace\\DataForRegressor.csv\")\n",
    "baseline_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=df.drop(['mean','STD','signalToNoise'], axis=1)\n",
    "data=df_data.dropna()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****************************ML*********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "def getRegressor(df,estimators,maxDepth,randomState):\n",
    "    X=df\n",
    "    y=df['global_score']\n",
    "    X_trainBase, X_testBase, y_train, y_test = train_test_split(X, y,random_state=1)\n",
    "    X_train=X_trainBase.drop(['chunk_or1','chunk_or2','global_score'], axis=1)\n",
    "    X_test=X_testBase.drop(['chunk_or1','chunk_or2','global_score'], axis=1)\n",
    "    model = RandomForestRegressor(n_estimators=estimators,max_depth=maxDepth, random_state=randomState)\n",
    "    model.fit(X_train,y_train)\n",
    "    start = time.time()\n",
    "    result = model.score(X_test,y_test)\n",
    "    end = time.time()\n",
    "    print(\"Score:\"+ str(result))\n",
    "    print(\"Time for executing model\")\n",
    "    print(end - start)\n",
    "    print(\" in seconds\")\n",
    "    return model,X_test,X_testBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,X_test,X_testBase=getRegressor(data,200,20,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testBase['globalAlign_pred']=model.predict(X_test)\n",
    "data = pd.DataFrame(X_testBase, columns=['globalAlign_pred'])\n",
    "baseline = pd.DataFrame(X_testBase, columns=['global_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mean'] = data['globalAlign_pred'].rolling(window=4).mean()\n",
    "data['STD'] = data['globalAlign_pred'].rolling(window=4).std()\n",
    "data['signalToNoise']=data['mean']/data['STD']\n",
    "data=data.fillna(0)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "baseline['mean'] =baseline['global_score'].rolling(window=4).mean()\n",
    "baseline['STD'] = baseline['global_score'].rolling(window=4).std()\n",
    "baseline['signalToNoise']=baseline['mean']/baseline['STD']\n",
    "baseline=baseline.fillna(0)\n",
    "baseline=scaler.fit_transform(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data_to_clusters(df):\n",
    "    from sklearn.cluster import KMeans\n",
    "    cluster = KMeans(n_clusters=3, init='k-means++', random_state=0)\n",
    "    return cluster.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "labelsAB=map_data_to_clusters(baseline)\n",
    "labelsAF=map_data_to_clusters(data)\n",
    "#print(cluster_ab_1['data_index'])\n",
    "'''here manually decid ewhich cluster is with P example and which is with negative examples'''\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "score=adjusted_rand_score(labelsAB,labelsAF)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyeasyga\n",
    "import time\n",
    "from pyeasyga import pyeasyga\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual, data):\n",
    "    temp=0;\n",
    "    cNum=1\n",
    "    for x in data:\n",
    "        if temp < x['score']:\n",
    "            temp=x['score']\n",
    "            cNum=x['clusterNum']\n",
    "    return cNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data\n",
    "print(X.head(5))\n",
    "start = time.time()\n",
    "#KMeans++\n",
    "arr=[];\n",
    "for i in range(2,20):\n",
    "    cluster_labels = KMeans(n_clusters=i, init='k-means++').fit_predict(X)\n",
    "    arr.append({'clusterNum': i , 'score': silhouette_score(X, cluster_labels)})\n",
    "    print({'clusterNum': i , 'score': silhouette_score(X, cluster_labels)})\n",
    "#initialise the GA with data\n",
    "ga = pyeasyga.GeneticAlgorithm(arr)\n",
    "ga.fitness_function = fitness               # set the GA's fitness function\n",
    "ga.run()                                    # run the GA\n",
    "print(ga.best_individual())\n",
    "end = time.time()\n",
    "print(\"Time for executing Kmeans++ with GA\")\n",
    "print(end - start)\n",
    "print(\" in seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
